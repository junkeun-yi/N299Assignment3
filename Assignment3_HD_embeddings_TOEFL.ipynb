{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0a3ff8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load modules and libraries\n",
    "import numpy as np\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b477c22",
   "metadata": {},
   "source": [
    "# Function to generate \"index\" vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ebcf7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_word_vector(dimension, k):\n",
    "    #k is a number of +1s and -1s if you decide to implement Random Indexing algorithm. So the total number of nonzero components in the vector is 2k\n",
    "    #For BEAGLE k is not relevant\n",
    "    v = np.array(np.zeros(dimension),np.int8) #Vector in initialized. \n",
    "    \n",
    "    #ToDo Generate components of an \"index\" vector randomly using the randomness suitable for the chosen algorithm\n",
    "    fill_ind = np.random.permutation(np.arange(0, dimension, 1))[:k]\n",
    "    random_ones_and_negs = np.random.binomial(1, 0.5, k) * 2 - 1\n",
    "    v[fill_ind] = random_ones_and_negs\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aed270",
   "metadata": {},
   "source": [
    "# Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5da17552",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"TOEFL_synonyms.txt\" # file with TOEFL dataset\n",
    "data_file_name = \"Glemmatized.txt\" # file with the text corpus\n",
    "\n",
    "dimension = 2000 # Dimensionality for high-dimensional vectors\n",
    "\n",
    "threshold = 100000 # Frequency threshold in the corpus \n",
    "\n",
    "ones_number = 5 # number of nonzero elements in randomly generated high-dimensional vectors\n",
    "window_size = 3 #number of neighboring words to consider both back and forth. In other words number of words before/after current word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621d1144",
   "metadata": {},
   "source": [
    "# Initialize \"index\" vectors and embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be004d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_dictionaries(data_file_name, dimension, threshold, ones_number):\n",
    "    dictionary = {} # vocabulary and corresponing random high-dimensional vectors\n",
    "    amount_dictionary = {} # counts frequency of words\n",
    "    word_space = {} # stores embedings\n",
    "\n",
    "    # Count how many times each word appears in the corpus\n",
    "    text_file = open(data_file_name, \"r\")\n",
    "    for line in text_file:\n",
    "        if line != \"\\n\":\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                if amount_dictionary.get(word) is None:\n",
    "                    amount_dictionary[word] = 1\n",
    "                else:\n",
    "                    amount_dictionary[word] += 1\n",
    "    text_file.close()\n",
    "\n",
    "    #Create a dictionary with the assigned random high-dimensional vectors\n",
    "    text_file = open(data_file_name, \"r\")\n",
    "    for line in text_file: #read line in the file\n",
    "        words = line.split() # extract words from the line\n",
    "        for word in words:  # for each word\n",
    "            if dictionary.get(word) is None: # If the word was not yed added to the vocabulary\n",
    "                if amount_dictionary[word] < threshold:\n",
    "                    dictionary[word] = get_random_word_vector(dimension, ones_number) # assign an \"index\" vector \n",
    "                else:\n",
    "                    dictionary[word] = np.zeros(dimension) # frequent words are assigned with empty vectors. In a way they will not contribute to the word embedding\n",
    "    text_file.close()\n",
    "    return dictionary, amount_dictionary, word_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c555c",
   "metadata": {},
   "source": [
    "# Choose embeddigns to construct  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "440e209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_toefl(test_name, dimension, word_space): \n",
    "    #Note that in order to save time we only create embeddings for the words needed in the TOEFL task\n",
    "    number_of_tests = 0\n",
    "    TOEFL_file = open(test_name, \"r\") # open TOEFL file\n",
    "\n",
    "    #Find all unique words amongst TOEFL tasks and initialize their embeddings to zeros    \n",
    "    for line in TOEFL_file:\n",
    "            words = line.split()\n",
    "            word_space[words[0]] = np.zeros(dimension)\n",
    "            word_space[words[1]] = np.zeros(dimension)\n",
    "            word_space[words[2]] = np.zeros(dimension)\n",
    "            word_space[words[3]] = np.zeros(dimension)\n",
    "            word_space[words[4]] = np.zeros(dimension)\n",
    "            number_of_tests += 1 # counts the number of test cases in TOEFL file\n",
    "    TOEFL_file.close()\n",
    "    return word_space, number_of_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1428e54",
   "metadata": {},
   "source": [
    "# Construct embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "343d4ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def create_embeddings(data_file_name, dimension, window_size, dictionary, word_space, mode=0):\n",
    "    #Each line in the corpus is a sentence so we only consider the window of words within the sentence.\n",
    "\n",
    "    text_file = open(data_file_name, \"r\")\n",
    "    line = \"placeholder\"\n",
    "    \n",
    "    while line != \"\":\n",
    "        line = text_file.readline()\n",
    "        words = line.split()\n",
    "        for i in range(0,len(words)):\n",
    "            if not (word_space.get(words[i]) is None): # This line forces us to create only embeddigns for words present in TOEFL\n",
    "                word_space[words[i]]\n",
    "                if mode == 0 or mode == 1:\n",
    "                    #Form \"context\" vector\n",
    "                    context=np.zeros(dimension) # initialize context vector\n",
    "                    for j in range(max(i-window_size,0),min(i+window_size+1,len(words))): # align window size with the location of the focus word in the sentence\n",
    "                        #ToDo increment context vector with the corresponding \"index\" vectors\n",
    "                        #Note that the index\" vector for the focus word in nor included into the context vector\n",
    "                        if j != i:\n",
    "                            context += dictionary.get(words[j])\n",
    "                    word_space[words[i]] += context # update the embedding with new context vector        \n",
    "                 \n",
    "                if mode == 0 or mode == 2:\n",
    "                    #Form \"order\" vector\n",
    "                    order=np.zeros(dimension) # initialize order vector\n",
    "                    for j in range(max(i-window_size,0),min(i+window_size+1,len(words))): # align window size with the location of the focus word in the sentence\n",
    "                        #ToDo increment context vector with the properly permuted \"index\" vectors\n",
    "                        if j != i:\n",
    "                            roll_amt = j-i # shift vector as permutation. inverse in shift negative\n",
    "                            order += np.roll(dictionary.get(words[j]), roll_amt)\n",
    "                    word_space[words[i]] += order # update the embedding with new order vector\n",
    "        \n",
    "    return word_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b0a6d",
   "metadata": {},
   "source": [
    "# Testing of the embeddings on TOEFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "16834e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to check if the answer for TOEFL synonyms task is correct\n",
    "def get_answer_mod(words):\n",
    "    min_value = min(spatial.distance.cosine(words[0], words[1]), spatial.distance.cosine(words[0], words[2]), spatial.distance.cosine(words[0], words[3]),\n",
    "                    spatial.distance.cosine(words[0], words[4]))\n",
    "    if min_value == spatial.distance.cosine(words[0],words[1]):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "682d18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracies(test_name, dimension, number_of_tests, amount_dictionary, word_space):\n",
    "    zero_vector = np.zeros(dimension) # used to check if an embedding is non empty \n",
    "    i = 0\n",
    "    TOEFL_file = open(test_name, 'r')\n",
    "    right_answers = 0.0 # variable for correct answers\n",
    "    number_skipped_tests = 0.0 # some tests could be skipped if there are no corresponding words in the vocabulary extracted from the training corpus\n",
    "    while i < number_of_tests:\n",
    "            line = TOEFL_file.readline() #read line in the file\n",
    "            words = line.split()  # extract words from the line\n",
    "            try:\n",
    "                if not(amount_dictionary.get(words[0]) is None): # check if there word in the corpus for the query word\n",
    "                    k = 1\n",
    "                    while k < 5:\n",
    "                        if np.array_equal(word_space[words[k]], zero_vector): # if no representation was learnt assign a random vector\n",
    "                            word_space[words[k]] = np.random.randn(dimension)\n",
    "                        k += 1\n",
    "                    right_answers += get_answer_mod([word_space[words[0]],word_space[words[1]],word_space[words[2]],\n",
    "                                word_space[words[3]],word_space[words[4]]]) #check if word is predicted right\n",
    "            except KeyError: # if there is no representation for the query vector than skip\n",
    "                number_skipped_tests += 1\n",
    "                print(\"skipped test: \" + str(i) + \"; Line: \" + str(words))\n",
    "            except IndexError:\n",
    "                break\n",
    "            i += 1\n",
    "    TOEFL_file.close()\n",
    "    accuracy = 100 * right_answers / number_of_tests # accuracy of the embeddings  \n",
    "    # print(\"Dimensionality of embeddings: \" +str(dimension) + \"; Percentage of correct answers in TOEFL: \" + str(accuracy) + \"%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "39c26037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(dimensionality, window_sizes, mode):\n",
    "    test_name = \"TOEFL_synonyms.txt\"\n",
    "    data_file_name = \"Glemmatized.txt\"\n",
    "    threshold = 100000\n",
    "    ones_number = 5\n",
    "    \n",
    "    results = []\n",
    "    for d in dimensionality:\n",
    "        dimension = d # Dimensionality for high-dimensional vectors\n",
    "\n",
    "        dictionary, amount_dictionary, d_word_space = populate_dictionaries(data_file_name, dimension, threshold, ones_number)\n",
    "        d_word_space, number_of_tests = process_toefl(test_name, dimension, d_word_space)\n",
    "        for ws in window_sizes:\n",
    "            ws_word_space = copy.deepcopy(d_word_space)\n",
    "            print(d, ws, mode)\n",
    "            window_size = ws #number of neighboring words to consider both back and forth. In other words number of words before/after current word\n",
    "\n",
    "            ws_word_space = create_embeddings(data_file_name, dimension, window_size, dictionary, ws_word_space, mode)\n",
    "            acc = compute_accuracies(test_name, dimension, number_of_tests, amount_dictionary, ws_word_space)\n",
    "            results.append((d, ws, acc))\n",
    "\n",
    "            print(\"Dimensionality of embeddings: \" +str(d) +  \"; Window Size: \" +str(ws) + \"; Percentage of correct answers in TOEFL: \" + str(acc) + \"%\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f380d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    for res in results:\n",
    "        print(f\"dim: {res[0]}, window_size: {res[1]}, accuracy: {res[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2eea6bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by dimensions\n",
      "256 3 0\n",
      "Dimensionality of embeddings: 256; Window Size: 3; Percentage of correct answers in TOEFL: 55.0%\n",
      "512 3 0\n",
      "Dimensionality of embeddings: 512; Window Size: 3; Percentage of correct answers in TOEFL: 53.75%\n",
      "768 3 0\n",
      "Dimensionality of embeddings: 768; Window Size: 3; Percentage of correct answers in TOEFL: 57.5%\n",
      "iter:0\n",
      "dim: 256, window_size: 3, accuracy: 55.0\n",
      "dim: 512, window_size: 3, accuracy: 53.75\n",
      "dim: 768, window_size: 3, accuracy: 57.5\n"
     ]
    }
   ],
   "source": [
    "# dimensionality_test\n",
    "dimensionality = [256, 512, 768]\n",
    "window_sizes = [3]\n",
    "mode=0\n",
    "print(\"Accuracy by dimensions\")\n",
    "d_results = run_test(dimensionality, window_sizes, mode)\n",
    "print(f\"iter:{i}\")\n",
    "print_results(d_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "819af040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 1 0\n",
      "Dimensionality of embeddings: 256; Window Size: 1; Percentage of correct answers in TOEFL: 48.75%\n",
      "256 2 0\n",
      "Dimensionality of embeddings: 256; Window Size: 2; Percentage of correct answers in TOEFL: 52.5%\n",
      "256 3 0\n",
      "Dimensionality of embeddings: 256; Window Size: 3; Percentage of correct answers in TOEFL: 52.5%\n",
      "256 4 0\n",
      "Dimensionality of embeddings: 256; Window Size: 4; Percentage of correct answers in TOEFL: 53.75%\n",
      "Accuracy by window size\n",
      "dim: 256, window_size: 1, accuracy: 48.75\n",
      "dim: 256, window_size: 2, accuracy: 52.5\n",
      "dim: 256, window_size: 3, accuracy: 52.5\n",
      "dim: 256, window_size: 4, accuracy: 53.75\n"
     ]
    }
   ],
   "source": [
    "# window_size test\n",
    "dimensionality = [256]\n",
    "window_sizes = [1, 2, 3, 4]\n",
    "mode=0\n",
    "ws_results = run_test(dimensionality, window_sizes, mode)\n",
    "print(\"Accuracy by window size\")\n",
    "print_results(ws_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e5a3c49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 3 1\n",
      "Dimensionality of embeddings: 256; Window Size: 3; Percentage of correct answers in TOEFL: 52.5%\n",
      "512 3 1\n",
      "Dimensionality of embeddings: 512; Window Size: 3; Percentage of correct answers in TOEFL: 52.5%\n",
      "768 3 1\n",
      "Dimensionality of embeddings: 768; Window Size: 3; Percentage of correct answers in TOEFL: 57.5%\n",
      "Accuracy with only context embeddings\n",
      "dim: 256, window_size: 3, accuracy: 52.5\n",
      "dim: 512, window_size: 3, accuracy: 52.5\n",
      "dim: 768, window_size: 3, accuracy: 57.5\n"
     ]
    }
   ],
   "source": [
    "# context_only test\n",
    "dimensionality = [256, 512, 768]\n",
    "window_sizes = [3]\n",
    "mode=1\n",
    "context_results = run_test(dimensionality, window_sizes, mode)\n",
    "print(\"Accuracy with only context embeddings\")\n",
    "print_results(context_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b5071156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 3 2\n",
      "Dimensionality of embeddings: 256; Window Size: 3; Percentage of correct answers in TOEFL: 50.0%\n",
      "512 3 2\n",
      "Dimensionality of embeddings: 512; Window Size: 3; Percentage of correct answers in TOEFL: 53.75%\n",
      "768 3 2\n",
      "Dimensionality of embeddings: 768; Window Size: 3; Percentage of correct answers in TOEFL: 58.75%\n",
      "Accuracy with order embeddings\n",
      "dim: 256, window_size: 3, accuracy: 50.0\n",
      "dim: 512, window_size: 3, accuracy: 53.75\n",
      "dim: 768, window_size: 3, accuracy: 58.75\n"
     ]
    }
   ],
   "source": [
    "# order_only test\n",
    "dimensionality = [256, 512, 768]\n",
    "window_sizes = [3]\n",
    "mode=2\n",
    "order_results = run_test(dimensionality, window_sizes, mode)\n",
    "print(\"Accuracy with order embeddings\")\n",
    "print_results(order_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd74f22",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "The semantic vectors were embedded using the Random Indexing strategy. \n",
    "\n",
    "Only one experiment was run per configuration due to limitations in compute resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce6b4e",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "results:\n",
    "\n",
    "`dim: 256, window_size: 3, accuracy: 55.0\n",
    "dim: 512, window_size: 3, accuracy: 53.75\n",
    "dim: 768, window_size: 3, accuracy: 57.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da47e04",
   "metadata": {},
   "source": [
    "## Question 2a\n",
    "\n",
    "results:\n",
    "\n",
    "`dim: 256, \n",
    "window_size: 1, accuracy: 48.75\n",
    "window_size: 2, accuracy: 52.5\n",
    "window_size: 3, accuracy: 52.5\n",
    "window_size: 4, accuracy: 53.75`\n",
    "\n",
    "As the window size increased, there was a general trend for accuracy to also increase. However, there was no increase in accuracy from window sizes 2 to 3. This may be because the words that are at distance 2 from the probed word may be 'frequent' words, and thus no additional information was embedded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ab024",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "results:\n",
    "\n",
    "`window_size: 3,\n",
    "dim: 256, accuracy: 55.0\n",
    "dim: 512, accuracy: 53.75\n",
    "dim: 768, accuracy: 57.5`\n",
    "\n",
    "The size of the dimensions seems to not not affect the accuracy of TOEFL synonymy assessment. The accuracy fell from  256 dims to 512 dims, and rose again from 512 dims to 768 dims, which can be interpretted as the accuracy being within a interval despite the changes in dimensins. This seems to show that because we are using a sparse encoding of vectors, even a relatively small dimension is sufficient to create accurate word embeddings. However, this may be a flawed result because only one experiment was done for each dimensionality. In that case, it can be inferred that the increasing dimensions actually increase accuracy and that the decrease in accuracy in 256 to 512 dimensions was a rare error. This notion is reinforced when looking at context-only and order-only embedding accuracies, which display increases in accuracy as the numer of dimensions increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f3b58",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Accuracy with only context embeddings\n",
    "\n",
    "`dim: 256, normal: 55.0, context-only: 52.5, order-only: 50.0\n",
    "dim: 512, normal: 53.75, context-only: 52.5, order-only: 53.75\n",
    "dim: 768, normal: 57.5, context-only: 57.5, order-only: 58.751`\n",
    "\n",
    "Context only embeddings performed better in the 256 dimension while order-only embeddings performed better in the higher 512 and 768 dimensions. This may display that context embeddings are effective even in smaller dimensions while the effectiveness of order embeddings increases faster than context embeddings do as the number of dimensions increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a34ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
